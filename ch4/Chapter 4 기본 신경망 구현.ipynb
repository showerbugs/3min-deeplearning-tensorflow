{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 인공신경망의 작동 원리\n",
    "\n",
    "인공신경망은 뇌의 신경 세포인 뉴런의 동작원리에 기초하여 만들어졌다. 뉴런의 기본 원리는 다음과 같다.\n",
    "\n",
    "`가지돌기(입력) -> 축삭돌기(처리) -> 축삭말단(출력) -> 다음 뉴런의 가지돌기(입력)`\n",
    "\n",
    "![neuron](images/neuron.png)\n",
    "\n",
    "입력된 신호는 축삭돌기에서 신호가 약해지거나 해서 출력으로 이어지지 않을 수도 있고, 처음보다 더 강하게 전달될 수도 있다. 인간은 이런 뉴런을 수억개 조합해서 다양한 행동이나 판단을 수행한다. 위의 개념을 이용해서 인공 뉴런을 다음과 같이 만들 수 있다.\n",
    "\n",
    "`output = ActivationFunction(input * weight + bias)`\n",
    "\n",
    "이것이 기본적인 인공 뉴런의 구조이다. 이 때, 더 좋은 출력`output`을 얻기 위해서 가중치`weight`와 편향`bias`을 찾아가는 최적화 과정을 학습이라고 한다.\n",
    "\n",
    "## 활성화 함수 Activation Function\n",
    "\n",
    "인공 뉴런을 통해 계산된 값을 최종적으로 어떤 값으로 만들지 결정하는 함수를 말한다. 대표적으로 시그모이드`Sigmoid`, 렐루`ReLU`, 쌍곡탄젠트`tanh` 등이 있다. 모양새는 다음과 같다.\n",
    "\n",
    "![activation-function](images/activation-function.png)\n",
    "\n",
    "## 신경망의 학습\n",
    "\n",
    "신경망을 학습시키려면 각 뉴런의 가중치와 편향 값을 일일이 변경해야 하는데, 신경망의 층이 깊어지고 뉴런이 많아질수록 이런 방법으로 학습을 진행하는 것은 현실적으로 불가능에 가깝다. 연구가 진행되고 다음과 같은 기법들이 발견되면서 현재는 큰 신경망에서도 효율적으로 학습이 가능하게 되었다.\n",
    "\n",
    "- 제한된 볼츠만 머신`Restricted Boltzmann Machine, RBM` 알고리즘\n",
    "- 드롭아웃`Dropout` 기법\n",
    "- 렐루`ReLU` 활성화 함수\n",
    "- 역전파`Backpropagation`\n",
    "\n",
    "이 중 역전파는 출력으로 나온 결과의 오차를 가지고, 신경망을 거꾸로 올라가면서 가중치와 편향을 다시 계산하고 저장해 나가는 방식이다. 기존의 입력층으로부터 계산해 나가는 기존 방식보다 더 빠르고 정확한 최적화가 가능하다.\n",
    "\n",
    "> 제한된 볼츠만 머신이나 역전파의 경우 이 책에서는 설명이 거의 없다. 드롭아웃은 뒤에서 다시 설명한다.\n",
    "\n",
    "텐서플로우에서는 위의 기본적인 개념이 다 구현되어 있기 때문이 따로 개발자가 구현해야 될 내용은 거의 없다.\n",
    "\n",
    "# 4.2 간단한 분류 모델 구현하기\n",
    "\n",
    "딥러닝이 가장 많이 쓰이는 분야 중 하나는 이미지의 패턴을 인식하는 분류`Classification`작업이다. 이번 예제에서는 털과 날개가 있느냐를 기준으로 포유류와 조류를 구분하는 신경망 모델을 만들어본다. 개념 이해를 돕기 위해서, 실제 이미지가 아니라 이진 데이터를 활용한다.\n",
    "\n",
    "## 학습 데이터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np  # 수치해석용 라이브러리, 행렬 조작/연산에 필수, 텐서플로우 내부에서도 긴밀하게 사용\n",
    "\n",
    "# [털, 날개], 있으면 1, 없으면 0\n",
    "feature_data = np.array([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "])\n",
    "\n",
    "# 각 개체의 실제 종류를 나타내는 레이블 (이런 식의 표기법을 '원-핫 인코딩'이라고 한다.)\n",
    "label_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 모델 구성\n",
    "\n",
    "다음으로, 특징 `feature_data`와 레이블 `label_data`의 관계를 알아내는 신경망 모델을 구성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = tf.placeholder(tf.float32)\n",
    "label = tf.placeholder(tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.random_uniform([2, 3], -1., 1.))  # 입력층(특징) 2 * 출력층(레이블) 3\n",
    "bias = tf.Variable(tf.zeros([3]))  # 출력층(레이블) 3\n",
    "\n",
    "L = tf.nn.relu(tf.add(tf.matmul(feature, weight), bias))\n",
    "\n",
    "model = tf.nn.softmax(L)  # 배열 내의 결과 합이 1이 되도록 만들어준다. 전체가 1이면 각각의 값을 확률이라고 생각할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "끝이다. 이 신경망을 그림으로 나타내면 다음과 같다.\n",
    "\n",
    "![artificial-neural-net](images/artificial-neural-net.png)\n",
    "\n",
    "## 손실 함수\n",
    "\n",
    "이제 손실 함수를 작성한다. 원-핫 인코딩 방법을 사용한 대부분의 모델에서 사용하는 교차 엔트로피`Cross-Entropy` 함수를 사용한다. 교차 엔트로피는 예측값과 실제값 사이의 확률 분포 차이를 계산한 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(label * tf.log(model), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수의 동작 과정을 자세히 살펴보면 다음과 같이 교차 엔트로피 값을 얻을 수 있다.\n",
    "\n",
    "![cost-function-1](images/cost-function-1.png)\n",
    "![cost-function-2](images/cost-function-2.png)\n",
    "![cost-function-3](images/cost-function-3.png)\n",
    "![cost-function-4](images/cost-function-4.png)\n",
    "\n",
    "## 학습\n",
    "\n",
    "마지막으로 텐서플로우가 제공하는 최적화 함수를 이용해서 학습을 시켜본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost of:  10 1.07705\n",
      "cost of:  20 1.07547\n",
      "cost of:  30 1.07399\n",
      "cost of:  40 1.07245\n",
      "cost of:  50 1.071\n",
      "cost of:  60 1.0695\n",
      "cost of:  70 1.06802\n",
      "cost of:  80 1.06664\n",
      "cost of:  90 1.06566\n",
      "cost of:  100 1.06468\n",
      "prediction value:  [0 1 1 0 0 0]\n",
      "actual value:  [0 1 2 0 0 2]\n",
      "accuracy:  66.6667\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)  # 경사하강법\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "# 100번의 학습 iteration\n",
    "for step in range(100):\n",
    "    session.run(train_op, feed_dict={\n",
    "        feature: feature_data,\n",
    "        label: label_data,\n",
    "    })\n",
    "    \n",
    "    # 10번마다 손실 출력\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print('cost of: ', step + 1, session.run(cost, feed_dict={\n",
    "            feature: feature_data,\n",
    "            label: label_data,\n",
    "        }))\n",
    "\n",
    "# 예측값과 실제값 출력\n",
    "prediction = tf.argmax(model, axis=1)  # 가장 큰 출력을 만드는 입력을 리턴\n",
    "actual = tf.argmax(label, axis=1)\n",
    "print('prediction value: ', session.run(prediction, feed_dict={\n",
    "    feature: feature_data,\n",
    "}))\n",
    "print('actual value: ', session.run(actual, feed_dict={\n",
    "    label: label_data,\n",
    "}))\n",
    "\n",
    "# 정확도 출력\n",
    "is_correct = tf.equal(prediction, actual)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('accuracy: ', session.run(accuracy * 100, feed_dict={\n",
    "    feature: feature_data,\n",
    "    label: label_data,\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 심층 신경망 구성하기\n",
    "\n",
    "정확도를 높이기 위해서 신경망의 층을 하나 늘려보자. 새로 늘어난 신경망의 뉴런의 수는 10개로 정해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "weight2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "bias1 = tf.Variable(tf.zeros([10]))\n",
    "bias2 = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 신경망의 연산 과정을 그림으로 나타내면 다음과 같다.\n",
    "\n",
    "![hidden-layer](images/hidden-layer.png)\n",
    "\n",
    "10개의 뉴런이 있는 중간 신경망을 은닉층`Hidden Layer`라고 한다. 은닉층의 적절한 뉴런 수도 하이퍼파라메터이므로 실험을 통해 찾아가야 한다.\n",
    "\n",
    "이제 방금 만든 가중치와 편향으로 뉴런을 구성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = tf.nn.relu(tf.add(tf.matmul(feature, weight1), bias1))\n",
    "model = tf.add(tf.matmul(L1, weight2), bias2)  # 왜 여기는 ReLU를 적용하지 않을까? 실제로 적용해보면 성능이 더 떨어짐..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 손실 함수를 작성해보자. 이번에도 교차 엔트로피 함수를 사용하지만, 텐서플로우가 제공하는 교차 엔트로피 함수를 이용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 작성한 뉴런 구성과 손실 함수를 이용해서 다시 한 번 학습을 진행해보자. 이번에는 최적화 방법으로 경사하강법이 아니라 보편적으로 성능이 더 좋다고 알려진 `AdamOptimizer`를 사용해 본다. 물론 모든 경우에 다 좋은 것은 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost of:  10 0.798288\n",
      "cost of:  20 0.648935\n",
      "cost of:  30 0.521132\n",
      "cost of:  40 0.403939\n",
      "cost of:  50 0.30709\n",
      "cost of:  60 0.226787\n",
      "cost of:  70 0.16453\n",
      "cost of:  80 0.120015\n",
      "cost of:  90 0.0888051\n",
      "cost of:  100 0.0672267\n",
      "prediction value:  [0 1 2 0 0 2]\n",
      "actual value:  [0 1 2 0 0 2]\n",
      "accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "# 100번의 학습 iteration\n",
    "for step in range(100):\n",
    "    session.run(train_op, feed_dict={\n",
    "        feature: feature_data,\n",
    "        label: label_data,\n",
    "    })\n",
    "    \n",
    "    # 10번마다 손실 출력\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print('cost of: ', step + 1, session.run(cost, feed_dict={\n",
    "            feature: feature_data,\n",
    "            label: label_data,\n",
    "        }))\n",
    "\n",
    "# 예측값과 실제값 출력\n",
    "prediction = tf.argmax(model, axis=1)  # 가장 큰 출력을 만드는 입력을 리턴\n",
    "actual = tf.argmax(label, axis=1)\n",
    "print('prediction value: ', session.run(prediction, feed_dict={\n",
    "    feature: feature_data,\n",
    "}))\n",
    "print('actual value: ', session.run(actual, feed_dict={\n",
    "    label: label_data,\n",
    "}))\n",
    "\n",
    "# 정확도 출력\n",
    "is_correct = tf.equal(prediction, actual)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('accuracy: ', session.run(accuracy * 100, feed_dict={\n",
    "    feature: feature_data,\n",
    "    label: label_data,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
