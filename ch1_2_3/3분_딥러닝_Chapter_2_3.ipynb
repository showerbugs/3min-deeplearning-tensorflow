{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. 텐서플로 설치와 주피터 노트북\n",
    "\n",
    "## 2.1 파이썬 및 필수 라이브러리 설치하기\n",
    "\n",
    "```sh\n",
    "$ pip install --upgrade tensorflow\n",
    "$ pip install install numpy matplotlib pillow\n",
    "```\n",
    "\n",
    "## 2.3 주피터 노트북\n",
    "\n",
    "웹 브라우저상에서 파이썬 코드를 단계적으로 쉽게 실행하고, 시각적으로 빠르게 확인해 볼 수 있도록 해주는 프로그램입니다.\n",
    "\n",
    "```sh\n",
    "$ pip3 install jupyter\n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. 텐서플로 프로그래밍 101\n",
    "\n",
    "tensorflow는 그래프 형태의 수학식 계산을 수행하는 핵심 라이브버리 위에 딥러닝을 포함한 여러 머신러닝을 쉽게 할 수 있는 라이브러리들을 올린 형태로 구성되어있습니다.\n",
    "\n",
    "![https://image.slidesharecdn.com/tensorflowkeras-170818142907/95/tensorflow-and-keras-an-overview-6-638.jpg?cb=1503066611](https://image.slidesharecdn.com/tensorflowkeras-170818142907/95/tensorflow-and-keras-an-overview-6-638.jpg?cb=1503066611)\n",
    "\n",
    "이를 위해 텐서 플로는 일반적인 프로그래밍과는 다른 개념을 몇개 포함합니다.\n",
    "\n",
    "- tensor\n",
    "- placeholder\n",
    "- variable\n",
    "- operation\n",
    "- graph 실행\n",
    "\n",
    "## 3.1 tensor와 graph 실행\n",
    "\n",
    "말로 해도 못 알아 먹을거 같으니 코드를 입력해가며 해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)\n",
    "# hello 변수는 Tensor라는 자료형이고 상수를 담고 있다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor는 tensorflow에서 다양한 수학식을 계산하기 위한 가장 기본적이고 중요한 자료형이다.\n",
    "\n",
    "tensorflow에서는 matrix 연산을 빈번하게 수행합니다. 그렇기에 자료형 tensor에서는 기본적으로 matrix를 지원합니다.\n",
    "\n",
    "- Rank : 차원의 수를 나타냅니다.\n",
    "    - Rank가 0이면 Scala\n",
    "    - 1이면 Vector\n",
    "    - 2이면 Matrix, 행렬\n",
    "    - 3 이상이면 n-Tensor\n",
    "- Shape : 각 차원의 element 개수, 즉 tensor의 구조 설명\n",
    "- dtype : tensor에 담긴 element들의 자료형, string, float, int 등\n",
    "\n",
    "![image/3_tensor.png](image/3_tensor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Const_2:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 프로그램과 다르게 tensorflow는 다음의 두 가지로 분리되어 있습니다.\n",
    "\n",
    "1. Graph 생성\n",
    "2. Graph 실행\n",
    "\n",
    "![image/3_tensorflow_program.png](image/3_tensorflow_program.png)\n",
    "\n",
    "Graph는 Tensor들의 연산 모음입니다. \n",
    "\n",
    "- tensorflow는 tensor와 tensor의 연산들을 먼저 정의하여 Graph를 만들고\n",
    "- 이후 필요할 때 연산을 실행하는 코드를 넣어 **원하는 시점**에 실제 연산을 수행\n",
    "\n",
    "> 이러한 방식을 lazy evaluation이라고 하며 functional programming에서 많이 사용됨\n",
    "\n",
    "이런 방식을 통해 실제 계산은 C++로 구현한 코어 라이브러리에서 수행하므로 높은 성능을 보여줍니다. 또한 모델 구성과 실행을 분리해 프로그램을 깔끔하게 작성할 수 있습니다.\n",
    "\n",
    "Graph의 실행은 Session 안에서 이뤄져야 하며 run 메서드를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_3:0\", shape=(), dtype=string)\n",
      "Tensor(\"Add_1:0\", shape=(), dtype=int32)\n",
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)\n",
    "print(c)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Placeholder와 Variable\n",
    "\n",
    "Placeholder는 Graph에 사용할 입력값을 나중에 받기 위해 사용하는 Parameter로 사용되는 Tensor입니다.\n",
    "\n",
    "Variable은 Graph를 최적화하는 용도로 Tensorflow가 학습한 결과를 갱신하기 위해 사용하는 Tensor입니다. 이 Variable의 값들이 Neural Network의 성능을 좌우합니다.\n",
    "\n",
    "![https://irenelizihui.files.wordpress.com/2016/05/13219756_798986766903560_704317321_n.jpg?w=840](https://irenelizihui.files.wordpress.com/2016/05/13219756_798986766903560_704317321_n.jpg?w=840)\n",
    "\n",
    "먼저 Placeholder를 먼저 사용해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# None은 아직 크기가 정해지지 않았다는 의미\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나중에 Placeholder X에 넣을 자료를 정의해보겠습니다.\n",
    "\n",
    "앞에서 tensor 모양을 (?, 3)으로 정의했으므로 두 번째 차원은 3개의 element를 가지고 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 3], [4, 5, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 Variable을 정의해보겠습니다\n",
    "\n",
    "- W는 [3, 2] Matrix 형태의 Tensor\n",
    "- b는 [2, 1] Matrix 형태의 Tensor\n",
    "\n",
    "normal distribution의 무작위 값으로 초기화합니다.\n",
    "\n",
    "물론 `tf.Variable([[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]])` 처럼 지정할 수도 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 수식을 작성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 Matrix Multiply에 대해서 하나 알아두면 좋습니다.\n",
    "\n",
    "- 행렬곱 A x B에 대하여, A의 Column(열)의 수와 B의 Row(행)의 수는 같아야 한다.\n",
    "- 행렬곱 A x B를 계산한 행렬 AB의 크기는 A의 Row(행) 개수와 B의 Column(열) 개수가 된다\n",
    "\n",
    "![image/3_matrix_muliply.png](image/3_matrix_muliply.png)\n",
    "\n",
    "X는 [2, 3] 이고 W는 [3, 2]이므로 곱한 결과의 형태는 [2, 2]가 된다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[-0.35214731 -1.3938725 ]\n",
      " [-0.48391855  0.54696041]\n",
      " [ 0.38931513 -0.2313734 ]]\n",
      "=== b ===\n",
      "[[-0.73635834]\n",
      " [ 0.83228809]]\n",
      "=== expr ===\n",
      "[[-0.15203905 -0.99407184]\n",
      " [-1.49229145 -4.22892857]]\n",
      "[[-0.8883974  -1.73043013]\n",
      " [-0.66000336 -3.39664054]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# variable을 초기화하는 함수\n",
    "# 기존에 학습한 값을 가져와서 사용하는 경우가 아니고 \n",
    "# 처음 실행할 때는, 연산 실행 전에 이 함수를 이용해 변수를 초기화 해야 한다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "# Graph를 실행할 때 feed_dict로 \n",
    "# Placeholder로 input data를 제공한다.\n",
    "print(sess.run(tf.matmul(X, W), feed_dict={X: x_data}))\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n",
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[-1.49374366 -1.87383628]\n",
      " [ 0.72828573 -2.28003216]\n",
      " [-0.72790271 -2.66243052]]\n",
      "=== b ===\n",
      "[[-0.72137994]\n",
      " [-1.62915206]]\n",
      "=== expr ===\n",
      "[[ -2.9422605  -15.1425724 ]\n",
      " [ -8.33011436 -36.49923706]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)\n",
    "\n",
    "x_data = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))\n",
    "\n",
    "expr = tf.matmul(X, W) + b\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Linear Regression 모델 구현하기\n",
    "\n",
    "Linear Regression이란 주어진 x와 y 값을 가지고 서로 간의 관계를 파악하는 것입니다.\n",
    "\n",
    "- 이 관계를 알고 나면 새로운 x 값이 주어졌을 때 y 값을 쉽게 알 수 있습니다.\n",
    "\n",
    "![image/3_linear_regression.png](image/3_linear_regression.png)\n",
    "\n",
    "이제 한번 Linear Regression 모델을 만들고 실행해봅시다.\n",
    "\n",
    "- 여기서는 x_data와 y_data의 상관관계를 파악해봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 x와 y의 상관관계를 설명하기 위한 Variable들인 W와 b를 무작위 값으로 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 자료를 입력 받을 Placeholder를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X와 Y의 상관관계를 분석하기 위한 가설(hypothesis)를 작성합니다.\n",
    "\n",
    "- 여기서는 X가 주어졌을 때 Y가 만들어지는 W와 b를 찾습니다.\n",
    "- W는 weight(가중치), b는 bias(편향)라 합니다.\n",
    "- 이 수식은 Linear Regression 부터 딥러닝까지 기본이 되는 수식입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 loss function(cost function)을 작성해보겠습니다.\n",
    "\n",
    "- loss function은 data에 대한 손실값를 계산하는 함수입니다.\n",
    "- 손실값란 실제 값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값\n",
    "- 즉 손실값이 작을수록 그 모델이 관계를 잘 설명하고 있다는 뜻입니다.\n",
    "- 이 손실을 전체 데이터에 대해 구한 경우 이를 cost라고 합니다.\n",
    "\n",
    "즉 Training이란 Variable의 값을 다양하게 넣어 계산하면서, 이 cost를 최소화하는 W와 b의 값을 구하는 것입니다.\n",
    "\n",
    "이 loss function으로는 **에측값과 실제 값의 거리**를 가장 많이 사용합니다.\n",
    "\n",
    "여기서는 유클리드 거리를 사용합니다\n",
    "\n",
    "- 모든 데이터로 `(예측값-실제값)^2` 를 구해 평균을 cost로 삼음\n",
    "\n",
    "![http://cfile5.uf.tistory.com/image/2329983A55CD88660CDD94](http://cfile5.uf.tistory.com/image/2329983A55CD88660CDD94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hypothesis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e63ebd803e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hypothesis' is not defined"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 gradient descent optimizer로 cost를 최소화하는 연산 Graph를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화 함수란 cost를 최소화하는 최적화된 Weight와 bias 값을 찾아주는 함수입니다.\n",
    "\n",
    "> 이 때 무작위로 변경하면서 찾으면 효율이 떨어집니다. 그래서 다양한 방법들을 사용합니다.\n",
    "\n",
    "Gradient Descent Algorithm은 최적화 방법 중 가장 기본적인 알고리즘입니다.\n",
    "\n",
    "loss function에서 기울기를 구하고 기울기가 낮은 쪽으로 계속 이동시킵니다. \n",
    "\n",
    "![https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png](https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png)\n",
    "\n",
    "learning_rate는 얼마나 학습을 할지를 결정하는 hyperparameter입니다.\n",
    "\n",
    "- 너무 크면 최적의 cost를 찾지 못하고 지나쳐버리거나 학습이 튀어버리고\n",
    "- 너무 작으면 속도가 매우 느려집니다.\n",
    "\n",
    "> **hyperparameter**\n",
    ">\n",
    "> Training을 진행하는 과정에 영향을 주는 변수를 의미하며, 이 값에 따라 학습 속도나 성능이 크게 달라질 수 있다. ML에서는 이 값을 잘 튜닝하는 것이 중요하다.\n",
    "\n",
    "이제 Linear Regression 모델을 다 만들었으니, Graph를 실행해 학습을 시킵니다.\n",
    "\n",
    "- 최적화를 수행하는 Graph인 train_op를 실행하고, 실행 시마다 변화하는 cost를 출력합니다.\n",
    "- 학습은 100번 수행하며, feed_dict로 x_data와 y_data를 전달합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fc23846d7cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, \n\u001b[0m\u001b[1;32m      6\u001b[0m                                                            Y: y_data})\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_op' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, \n",
    "                                                           Y: y_data})\n",
    "        if step % 10 is not 0: continue\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    \n",
    "    print(\"==\" * 30)\n",
    "    # 모델이 잘 동작하는지 확인해 보겠습니다.\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.04304 [ 0.55748618] [ 1.16262317]\n",
      "10 0.119726 [ 0.60778624] [ 0.89159399]\n",
      "20 0.0735928 [ 0.69249934] [ 0.69902128]\n",
      "30 0.0452358 [ 0.75891548] [ 0.5480417]\n",
      "40 0.0278054 [ 0.81098664] [ 0.42967176]\n",
      "50 0.0170913 [ 0.85181111] [ 0.33686826]\n",
      "60 0.0105056 [ 0.88381797] [ 0.26410908]\n",
      "70 0.00645755 [ 0.90891176] [ 0.20706496]\n",
      "80 0.0039693 [ 0.92858559] [ 0.16234164]\n",
      "90 0.00243984 [ 0.9440102] [ 0.12727796]\n",
      "============================================================\n",
      "X: 5, Y: [ 4.87735558]\n",
      "X: 2.5, Y: [ 2.48980069]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "\n",
    "hypothesis = W * X + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, \n",
    "                                                           Y: y_data})\n",
    "        if step % 10 is not 0: continue\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    \n",
    "    print(\"==\" * 30)\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
